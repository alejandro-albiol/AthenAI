{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6023fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --upgrade datasets transformers\n",
    "!pip install -U datasets==3.0.1 transformers==4.45.2\n",
    "!pip install -q transformers sentence-transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c531bf",
   "metadata": {},
   "source": [
    "# AthenAI v1 - Workout Generation Model Training\n",
    "\n",
    "This notebook documents the training process for the AthenAI workout generation model using T5 and LoRA adaptation.\n",
    "\n",
    "## 1. Setup and Memory Management\n",
    "First, we'll set up memory management and GPU monitoring utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dca88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory periodically\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "def cleanup_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Call this function between major steps\n",
    "cleanup_memory()\n",
    "\n",
    "# Monitor GPU memory\n",
    "def print_gpu_utilization():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "        print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401d4b9d",
   "metadata": {},
   "source": [
    "## 2. Package Installation and Imports\n",
    "Install required packages and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e738995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets torch accelerate peft bitsandbytes kaggle requests tqdm\n",
    "!pip install --upgrade huggingface_hub\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae293e98",
   "metadata": {},
   "source": [
    "## 3. Data Collection\n",
    "Download exercise data from GitHub and optionally from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41cea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GitHub exercise database\n",
    "def download_github_exercises():\n",
    "    \"\"\"Download and extract GitHub exercise database\"\"\"\n",
    "    url = \"https://github.com/yuhonas/free-exercise-db/archive/main.zip\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    with open(\"exercise_db.zip\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    with zipfile.ZipFile(\"exercise_db.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"./\")\n",
    "\n",
    "    # Load exercises\n",
    "    exercises_path = \"./free-exercise-db-main/exercises\"\n",
    "    exercises = []\n",
    "\n",
    "    for file in os.listdir(exercises_path):\n",
    "        if file.endswith('.json'):\n",
    "            with open(os.path.join(exercises_path, file), 'r') as f:\n",
    "                exercise = json.load(f)\n",
    "                exercises.append(exercise)\n",
    "\n",
    "    return exercises\n",
    "\n",
    "github_exercises = download_github_exercises()\n",
    "print(f\"Loaded {len(github_exercises)} exercises from GitHub\")\n",
    "\n",
    "# Setup Kaggle API and download dataset\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "def setup_kaggle_api():\n",
    "    \"\"\"Upload kaggle.json and configure Kaggle API\"\"\"\n",
    "    print(\"Please upload your kaggle.json file:\")\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "    for filename in uploaded.keys():\n",
    "        if filename == 'kaggle.json':\n",
    "            os.rename(filename, '/root/.kaggle/kaggle.json')\n",
    "            os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
    "            print(\"‚úÖ Kaggle API configured successfully!\")\n",
    "            return True\n",
    "\n",
    "    print(\"‚ùå kaggle.json not found in uploaded files\")\n",
    "    return False\n",
    "\n",
    "kaggle_configured = setup_kaggle_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79793ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Kaggle dataset if API is configured\n",
    "kaggle_exercises = None\n",
    "if kaggle_configured:\n",
    "    try:\n",
    "        !kaggle datasets download -d niharika41298/gym-exercise-data\n",
    "        if os.path.exists('gym-exercise-data.zip'):\n",
    "            !unzip -o gym-exercise-data.zip\n",
    "            print(\"‚úÖ Dataset downloaded and extracted!\")\n",
    "\n",
    "        csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
    "        if csv_files:\n",
    "            csv_file = csv_files[0]\n",
    "            kaggle_exercises = pd.read_csv(csv_file)\n",
    "            print(f\"‚úÖ Loaded Kaggle dataset: {csv_file} with {len(kaggle_exercises)} exercises\")\n",
    "            print(\"Dataset columns:\", list(kaggle_exercises.columns))\n",
    "        else:\n",
    "            print(\"‚ùå No CSV file found after extraction\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading Kaggle dataset: {e}\")\n",
    "        print(\"You can manually upload the CSV file instead\")\n",
    "        print(\"\\nAlternative: Upload CSV file manually:\")\n",
    "        uploaded_csv = files.upload()\n",
    "        for filename in uploaded_csv.keys():\n",
    "            if filename.endswith('.csv'):\n",
    "                kaggle_exercises = pd.read_csv(filename)\n",
    "                print(f\"‚úÖ Manually uploaded CSV loaded: {len(kaggle_exercises)} exercises\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf1a386",
   "metadata": {},
   "source": [
    "## 4. Data Standardization\n",
    "Standardize exercise data from both sources into a common format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8ceb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_exercise_data(github_exercises, kaggle_df=None):\n",
    "    \"\"\"\n",
    "    Standardize both datasets into a common format\n",
    "    \"\"\"\n",
    "    standardized_exercises = []\n",
    "\n",
    "    # Process GitHub exercises\n",
    "    for ex in github_exercises:\n",
    "        standardized_ex = {\n",
    "            'name': ex.get('name', ''),\n",
    "            'difficulty_level': ex.get('level', 'beginner'),\n",
    "            'exercise_type': ex.get('category', 'strength'),\n",
    "            'primary_muscles': ex.get('primaryMuscles', []),\n",
    "            'secondary_muscles': ex.get('secondaryMuscles', []),\n",
    "            'equipment': ex.get('equipment', ''),\n",
    "            'instructions': ' '.join(ex.get('instructions', [])),\n",
    "            'force_type': ex.get('force', ''),\n",
    "            'mechanic': ex.get('mechanic', ''),\n",
    "            'source': 'github'\n",
    "        }\n",
    "        standardized_exercises.append(standardized_ex)\n",
    "\n",
    "    # Process Kaggle exercises (if available)\n",
    "    if kaggle_df is not None:\n",
    "        for _, row in kaggle_df.iterrows():\n",
    "            standardized_ex = {\n",
    "                'name': row.get('Title', ''),\n",
    "                'difficulty_level': 'intermediate',\n",
    "                'exercise_type': 'strength',\n",
    "                'primary_muscles': [row.get('BodyPart', '')] if pd.notna(row.get('BodyPart')) else [],\n",
    "                'secondary_muscles': [],\n",
    "                'equipment': row.get('Equipment', ''),\n",
    "                'instructions': row.get('Desc', ''),\n",
    "                'force_type': '',\n",
    "                'mechanic': '',\n",
    "                'source': 'kaggle'\n",
    "            }\n",
    "            standardized_exercises.append(standardized_ex)\n",
    "\n",
    "    return standardized_exercises\n",
    "\n",
    "# Process the data\n",
    "standardized_exercises = standardize_exercise_data(github_exercises, kaggle_exercises)\n",
    "print(f\"Standardized {len(standardized_exercises)} exercises\")\n",
    "cleanup_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de44106",
   "metadata": {},
   "source": [
    "## 5. Training Data Preparation\n",
    "Create training prompts and responses for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9092a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_prompts(exercises):\n",
    "    \"\"\"Create training prompts for workout generation\"\"\"\n",
    "    training_data = []\n",
    "    user_profiles = [\n",
    "        {\n",
    "            'training_phase': 'weight_loss',\n",
    "            'motivation': 'self_improvement',\n",
    "            'special_situation': 'none',\n",
    "            'description': 'Beginner looking to lose weight'\n",
    "        },\n",
    "        {\n",
    "            'training_phase': 'muscle_gain',\n",
    "            'motivation': 'self_improvement',\n",
    "            'special_situation': 'none',\n",
    "            'description': 'Intermediate wanting to build muscle'\n",
    "        },\n",
    "        {\n",
    "            'training_phase': 'cardio_improve',\n",
    "            'motivation': 'wellbeing',\n",
    "            'special_situation': 'none',\n",
    "            'description': 'Advanced athlete improving cardio'\n",
    "        },\n",
    "        {\n",
    "            'training_phase': 'maintenance',\n",
    "            'motivation': 'wellbeing',\n",
    "            'special_situation': 'elderly_population',\n",
    "            'description': 'Senior maintaining fitness'\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for profile in user_profiles:\n",
    "        suitable_exercises = filter_exercises_for_profile(exercises, profile)\n",
    "        if len(suitable_exercises) < 5:\n",
    "            continue\n",
    "\n",
    "        prompt = f\"\"\"Generate a workout routine for:\n",
    "User Profile: {profile['description']}\n",
    "Training Phase: {profile['training_phase']}\n",
    "Motivation: {profile['motivation']}\n",
    "Special Situation: {profile['special_situation']}\n",
    "\n",
    "Available Exercises:\n",
    "{format_exercises_for_prompt(suitable_exercises[:10])}\n",
    "\n",
    "Generate a workout with the following structure:\n",
    "- Workout Name\n",
    "- Description\n",
    "- Difficulty Level\n",
    "- Estimated Duration\n",
    "- Target Audience\n",
    "- Blocks (Warmup, Main, Core, Cooldown)\n",
    "\n",
    "Workout:\"\"\"\n",
    "\n",
    "        response = generate_workout_response(suitable_exercises, profile)\n",
    "        training_data.append({\n",
    "            'prompt': prompt,\n",
    "            'response': response\n",
    "        })\n",
    "\n",
    "    return training_data\n",
    "\n",
    "def filter_exercises_for_profile(exercises, profile):\n",
    "    \"\"\"Filter exercises suitable for user profile\"\"\"\n",
    "    suitable = []\n",
    "    for ex in exercises:\n",
    "        if profile['training_phase'] == 'weight_loss' and ex['exercise_type'] in ['cardio', 'strength']:\n",
    "            suitable.append(ex)\n",
    "        elif profile['training_phase'] == 'muscle_gain' and ex['exercise_type'] == 'strength':\n",
    "            suitable.append(ex)\n",
    "        elif profile['special_situation'] == 'elderly_population' and ex['difficulty_level'] == 'beginner':\n",
    "            suitable.append(ex)\n",
    "        else:\n",
    "            suitable.append(ex)\n",
    "    return suitable[:20]\n",
    "\n",
    "def format_exercises_for_prompt(exercises):\n",
    "    \"\"\"Format exercises for inclusion in prompt\"\"\"\n",
    "    formatted = []\n",
    "    for ex in exercises[:5]:\n",
    "        formatted.append(f\"- {ex['name']}: {ex['exercise_type']} ({ex['difficulty_level']})\")\n",
    "    return '\\n'.join(formatted)\n",
    "\n",
    "def generate_workout_response(exercises, profile):\n",
    "    \"\"\"Generate a simple workout response template\"\"\"\n",
    "    return f\"\"\"Workout Name: {profile['training_phase'].title()} Focused Routine\n",
    "Description: A targeted workout designed for {profile['description'].lower()}\n",
    "Difficulty Level: {profile.get('difficulty', 'intermediate')}\n",
    "Estimated Duration: 45-60 minutes\n",
    "Target Audience: {profile['training_phase']}\n",
    "\n",
    "Blocks:\n",
    "1. Warmup Block (5-10 minutes)\n",
    "   - Light cardio and dynamic stretches\n",
    "\n",
    "2. Main Block (30-40 minutes)\n",
    "   - {exercises[0]['name']}: 3 sets x 10-12 reps\n",
    "   - {exercises[1]['name']}: 3 sets x 10-12 reps\n",
    "   - {exercises[2]['name']}: 3 sets x 8-10 reps\n",
    "\n",
    "3. Core Block (5-10 minutes)\n",
    "   - Core strengthening exercises\n",
    "\n",
    "4. Cooldown Block (5-10 minutes)\n",
    "   - Static stretching and relaxation\"\"\"\n",
    "\n",
    "# Generate training data\n",
    "training_data = create_training_prompts(standardized_exercises)\n",
    "print(f\"Generated {len(training_data)} training examples\")\n",
    "cleanup_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0cfd76",
   "metadata": {},
   "source": [
    "## 6. Model Configuration\n",
    "Load the pre-trained model and configure LoRA adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb0a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"a-albiol/AthenAI\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q\", \"v\", \"o\", \"wi_0\", \"wi_1\", \"wo\"],\n",
    "    bias=\"none\",\n",
    "    use_rslora=False,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.train()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# Print parameter analysis\n",
    "print(\"=== Model Parameter Analysis ===\")\n",
    "total_params = 0\n",
    "trainable_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    total_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "        print(f\"‚úÖ Trainable: {name} - {param.numel():,} params\")\n",
    "\n",
    "print(f\"\\nüìä Total parameters: {total_params:,}\")\n",
    "print(f\"üìä Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"üìä Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "cleanup_memory()\n",
    "print(\"‚úÖ Memory cleaned after model setup\")\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c87f496",
   "metadata": {},
   "source": [
    "## 7. Training Setup\n",
    "Prepare the dataset, training arguments, and trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27dd8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function_t5(examples):\n",
    "    \"\"\"Tokenize the training data for T5\"\"\"\n",
    "    inputs = [prompt for prompt in examples['prompt']]\n",
    "    targets = [response for response in examples['response']]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=512,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=512,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = Dataset.from_list(training_data)\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_function_t5,\n",
    "    batched=True,\n",
    "    remove_columns=['prompt', 'response']\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(train_dataset)}\")\n",
    "cleanup_memory()\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./workout-model-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    fp16=True,\n",
    "    report_to=[],\n",
    "    gradient_checkpointing=True,\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    pad_to_multiple_of=8,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Disable wandb\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "    wandb.init(mode=\"disabled\")\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac4f9b0",
   "metadata": {},
   "source": [
    "## 8. Model Training\n",
    "Train the model and save checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f752d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify model is ready\n",
    "print(\"=== Pre-training Verification ===\")\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "if trainable_params == 0:\n",
    "    raise ValueError(\"‚ùå No trainable parameters! Check LoRA configuration.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Ready to train with {trainable_params:,} parameters\")\n",
    "\n",
    "# Test forward pass\n",
    "print(\"üß™ Testing forward pass...\")\n",
    "try:\n",
    "    sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**sample_batch)\n",
    "    print(\"‚úÖ Forward pass successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Forward pass failed: {e}\")\n",
    "    print(\"üîß This might indicate a data formatting issue\")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "print_gpu_utilization()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "cleanup_memory()\n",
    "print(\"‚úÖ Memory cleaned after training\")\n",
    "print_gpu_utilization()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"./workout-model-final\")\n",
    "tokenizer.save_pretrained(\"./workout-model-final\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92186cd4",
   "metadata": {},
   "source": [
    "## 9. Inference and Testing\n",
    "Implement and test workout generation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac01d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_workout_t5(user_profile, available_exercises=None):\n",
    "    \"\"\"Generate a workout using T5 model\"\"\"\n",
    "    if available_exercises is None:\n",
    "        available_exercises = standardized_exercises[:5]\n",
    "\n",
    "    prompt = f\"\"\"Generate workout routine:\n",
    "User: {user_profile}\n",
    "Phase: muscle_gain\n",
    "Motivation: self_improvement\n",
    "Special: none\n",
    "Exercises: {format_exercises_for_prompt(available_exercises)}\n",
    "Format: name, description, difficulty, duration, blocks\"\"\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=300,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def generate_workout_simple(user_profile):\n",
    "    \"\"\"Simpler generation function\"\"\"\n",
    "    prompt = f\"Generate a workout for: {user_profile}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=256, truncation=True)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=512,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test generation\n",
    "print(\"=== Testing T5 Generation ===\")\n",
    "try:\n",
    "    print(\"üß™ Testing main generation function...\")\n",
    "    test_response = generate_workout_t5(\"Intermediate athlete wanting to build muscle\")\n",
    "    print(\"‚úÖ Main function works!\")\n",
    "    print(\"Generated Workout:\")\n",
    "    print(test_response)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Main function failed: {e}\")\n",
    "    print(\"üîÑ Trying simple function...\")\n",
    "    \n",
    "    try:\n",
    "        test_response = generate_workout_simple(\"Intermediate athlete wanting to build muscle\")\n",
    "        print(\"‚úÖ Simple function works!\")\n",
    "        print(\"Generated Workout:\")\n",
    "        print(test_response)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Simple function also failed: {e}\")\n",
    "        print(\"üîß Try running the model loading section again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbfe8f0",
   "metadata": {},
   "source": [
    "## 10. Model Deployment\n",
    "Save and upload the model to Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4684e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face\n",
    "from huggingface_hub import login\n",
    "login(ACCESS_TOKEN)  # Replace ACCESS_TOKEN with your actual token\n",
    "\n",
    "# Push model to hub\n",
    "model.push_to_hub(\"a-albiol/AthenAI\")\n",
    "tokenizer.push_to_hub(\"a-albiol/AthenAI\")\n",
    "\n",
    "print(\"‚úÖ Model successfully pushed to Hugging Face Hub\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
