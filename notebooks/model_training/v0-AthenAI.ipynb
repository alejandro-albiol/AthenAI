{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49d66b00",
   "metadata": {},
   "source": [
    "# AthenAI v0 - Initial Workout Generation Model\n",
    "\n",
    "This notebook contains the initial implementation of the AthenAI workout generation model using FLAN-T5 and sentence transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec33c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --upgrade datasets transformers\n",
    "!pip install -U datasets==3.0.1 transformers==4.45.2\n",
    "!pip install -q transformers sentence-transformers datasets\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import datasets\n",
    "import torch\n",
    "\n",
    "# Load FLAN-T5-base tokenizer and model for generation/fine-tuning\n",
    "t5_model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(t5_model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(t5_model_name)\n",
    "\n",
    "# Load embedding model for matching\n",
    "embed_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embed_model = SentenceTransformer(embed_model_name)\n",
    "\n",
    "print(\"Models loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fe4e4a",
   "metadata": {},
   "source": [
    "## Data Loading and Processing\n",
    "Load the exercise dataset and process it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da53b101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = datasets.load_dataset(\"onurSakar/GYM-Exercise\")\n",
    "print(\"Sample data point:\", dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d63168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_instruction_response(example):\n",
    "    \"\"\"\n",
    "    Extract instruction and response from text using regex.\n",
    "    \n",
    "    Args:\n",
    "        example: Dictionary containing the text field\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with instruction and response fields\n",
    "    \"\"\"\n",
    "    text = example['text']\n",
    "    # Extract instruction (between [INST] and [/INST] tokens)\n",
    "    instruction_match = re.search(r'\\[INST\\](.*?)\\[/INST\\]', text, re.DOTALL)\n",
    "    instruction_text = \"\"\n",
    "    if instruction_match:\n",
    "        # Remove internal tags like <<SYS>> ... <</SYS>>\n",
    "        inst_content = instruction_match.group(1)\n",
    "        inst_clean = re.sub(r'<<SYS>>.*?<</SYS>>', '', inst_content, flags=re.DOTALL).strip()\n",
    "        instruction_text = inst_clean\n",
    "\n",
    "    # Extract response after [/INST]\n",
    "    parts = re.split(r'\\[/INST\\]', text, maxsplit=1)\n",
    "    response_text = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "\n",
    "    return {\n",
    "        \"instruction\": instruction_text,\n",
    "        \"response\": response_text\n",
    "    }\n",
    "\n",
    "# Process the dataset\n",
    "processed_dataset = dataset['train'].map(extract_instruction_response)\n",
    "print(\"Processed sample:\", processed_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58dbe3a",
   "metadata": {},
   "source": [
    "## Model Training Configuration\n",
    "Set up training arguments and initialize the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29d2b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    num_train_epochs=1,\n",
    "    save_total_limit=1,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "print(\"Training configuration ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91f8dd2",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Train the model and upload it to the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login with token\n",
    "login(ACCESS_TOKEN)  # Replace ACCESS_TOKEN with your actual token\n",
    "\n",
    "# Upload model and tokenizer to Hub with name 'AthenAI'\n",
    "model.push_to_hub(\"AthenAI\")\n",
    "tokenizer.push_to_hub(\"AthenAI\")\n",
    "\n",
    "print(\"Model and tokenizer successfully uploaded to Hugging Face Hub as 'AthenAI'.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
